{
  
    
        "post0": {
            "title": "Pluto to ipynb",
            "content": "# v0.11.10 using Markdown using InteractiveUtils . begin using Random using Plots using HDF5 using Statistics using Base.Iterators end . Hey! This post is about my introduction to the world of Julia. I took this challenge of learning Julia and making something in it. Since Julia is pretty similar to Python, I made a hypothesis. That is can I learn julia and be up and running with something in two days? What I realised is, if you&#39;re from a python background and have some expereince in it, then learning Julia is going to be fun and breezy for you. So, here I am after my two day rendezvous with Julia. . So, what I used to learn Julia? I used resources from julia academy . What did I implement? I decided to go for one the resources I learnt deep learning from: Neural Networks and Deep Learning . Impemented the Julia version of Week 2 assignment of Neural Networks and Deep Learning course. . I hope it&#39;s useful to you. It was a lot of fun and I am in love with Julia â¤ . Let&#39;s begin! . Load dataset . There are two files: train_catvnoncat.h5 &amp; test_catvnoncat.h5 | According to our notation, X is of shape (num_features, num_examples) &amp; y is a row vector of shape (1, num_examples). | We write a function load_dataset() which: Takes in HDF5 files | Converts them into Array{Float64, 2} arrays. | Reshapes them according to our notation &amp; returns X_train, y_train, X_test, y_test | . | function load_dataset(train_file::String, test_file::String) X_train = convert(Array{Float64, 4}, h5read(train_file, &quot;train_set_x&quot;)) y_train = convert(Array{Float64, 1}, h5read(train_file, &quot;train_set_y&quot;)) X_test = convert(Array{Float64, 4}, h5read(test_file, &quot;test_set_x&quot;)) y_test = convert(Array{Float64, 1}, h5read(test_file, &quot;test_set_y&quot;)) num_features_train_X = size(X_train, 1) * size(X_train, 2) * size(X_train, 2) num_features_test_X = size(X_test, 1) * size(X_test, 2) * size(X_test, 2) X_train = reshape(X_train, (num_features_train_X, size(X_train, 4))) y_train = reshape(y_train, (1, size(y_train, 1))) X_test = reshape(X_test, (num_features_test_X, size(X_test, 4))) y_test = reshape(y_test, (1, size(y_test, 1))) X_train, y_train, X_test, y_test end . Normalization . begin X_train, y_train, X_test, y_test = load_dataset( raw&quot;C: Users Abhishek Swain Desktop Neural-Networks-and-Deep-Learning-in-Julia Week-2 train_catvnoncat.h5&quot;, raw&quot;C: Users Abhishek Swain Desktop Neural-Networks-and-Deep-Learning-in-Julia Week-2 test_catvnoncat.h5&quot;); X_train, X_test= X_train/255, X_test/255; @time size(X_train), size(y_train), size(X_test), size(y_test) end . Sigmoid . Applies sigmoid to the vector . function Ïƒ(z) &quot;&quot;&quot; Compute the sigmoid of z &quot;&quot;&quot; return one(z) / (one(z) + exp(-z)) end . Random initialization . Initialize w &amp; b with with random values between (0, 1) . function initialize(dim) &quot;&quot;&quot; This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. Argument: dim -- size of the w vector we want (or number of parameters in this case) Returns: w -- initialized vector of shape (dim, 1) b -- initialized scalar (corresponds to the bias) &quot;&quot;&quot; w = zeros(dim, 1) b = 2 @assert(size(w) == (dim, 1)) @assert(isa(b, Float64) || isa(b, Int64)) return w, b end . function propagate(w, b, X, Y) &quot;&quot;&quot; Implement the cost function and its gradient for the propagation explained above Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples) Return: cost -- negative log-likelihood cost for logistic regression dw -- gradient of the loss with respect to w, thus same shape as w db -- gradient of the loss with respect to b, thus same shape as b Tips: - Write your code step by step for the propagation &quot;&quot;&quot; m = size(X, 2) # Forward prop Z = w&#39;X .+ b yÌ‚ = Ïƒ.(Z) @assert(size(yÌ‚) == size(Y)) # Compute cost ğ’¥ = -1 * sum(Y .* log.(yÌ‚) .+ (1 .- Y) .* log.(1 .- yÌ‚)) ğ’¥ /= m @assert(size(ğ’¥) == ()) # Back-prop ğœ•ğ‘§ = yÌ‚ - Y @assert(size(ğœ•ğ‘§) == size(yÌ‚) &amp;&amp; size(ğœ•ğ‘§) == size(Y)) ğœ•ğ‘¤ = (1/m) * X * ğœ•ğ‘§&#39; ğœ•ğ‘ = (1/m) * sum(ğœ•ğ‘§) ğ’¥, Dict(&quot;ğœ•ğ‘¤&quot; =&gt; ğœ•ğ‘¤, &quot;ğœ•ğ‘&quot; =&gt; ğœ•ğ‘) end . function optimize(w, b, X, Y, num_iterations, ğ›¼, print_cost) &quot;&quot;&quot; This function optimizes w and b by running a gradient descent algorithm Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of shape (num_px * num_px * 3, number of examples) Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples) num_iterations -- number of iterations of the optimization loop learning_rate -- learning rate of the gradient descent update rule print_cost -- True to print the loss every 100 steps Returns: params -- dictionary containing the weights w and bias b grads -- dictionary containing the gradients of the weights and bias with respect to the cost function costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve. Tips: You basically need to write down two steps and iterate through them: 1) Calculate the cost and the gradient for the current parameters. Use propagate(). 2) Update the parameters using gradient descent rule for w and b &quot;&quot;&quot; costs = Array{Float64, 2}(undef, num_iterations, 1) for i=1:num_iterations ğ’¥, ğ›» = propagate(w, b, X, Y) ğœ•ğ‘¤, ğœ•ğ‘ = ğ›»[&quot;ğœ•ğ‘¤&quot;], ğ›»[&quot;ğœ•ğ‘&quot;] global ğœ•ğ‘¤, ğœ•ğ‘ w -= ğ›¼ .* ğœ•ğ‘¤ b -= ğ›¼ .* ğœ•ğ‘ costs[i] = ğ’¥ if print_cost &amp;&amp; i % 100 == 0 println(&quot;Cost after iteration $i = $ğ’¥&quot;) end end params = Dict(&quot;w&quot; =&gt; w, &quot;b&quot; =&gt; b) grads = Dict(&quot;ğœ•ğ‘¤&quot; =&gt; ğœ•ğ‘¤, &quot;ğœ•ğ‘&quot; =&gt; ğœ•ğ‘) params, grads, costs end . function predict(w, b, X) &quot;&quot;&quot; Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Returns: Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X &quot;&quot;&quot; m = size(X, 2) preds = zeros(1, m) yÌ‚ = Ïƒ.(w&#39;X .+ b) preds = [p &gt; 0.5 ? 1 : 0 for p in Iterators.flatten(yÌ‚)] preds = reshape(preds, (1, m)) @assert(size(preds) == (1, m)) preds end . Model . Combine all functions to train the model. Learning rate: $ alpha = 0.005$, iterations(epochs): 2000 . function model(X_train, y_train, X_test, y_test, num_iterations, ğ›¼, print_cost) &quot;&quot;&quot; Builds the logistic regression model by calling the function you&#39;ve implemented previously Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) num_iterations -- hyperparameter representing the number of iterations to optimize the parameters learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize() print_cost -- Set to true to print the cost every 100 iterations Returns: d -- dictionary containing information about the model. &quot;&quot;&quot; # Initialize parameters w, b = initialize(size(X_train, 1)) # Gradient descent params, grads, costs = optimize(w, b, X_train, y_train, num_iterations, ğ›¼, print_cost) w, b = params[&quot;w&quot;], params[&quot;b&quot;] preds_test = predict(w, b, X_test) preds_train = predict(w, b, X_train) train_acc = 100 - mean(abs.(preds_train - y_train)) * 100 test_acc = 100 - mean(abs.(preds_test - y_test)) * 100 @show train_acc @show test_acc d = Dict( &quot;costs&quot; =&gt; costs, &quot;test_preds&quot; =&gt; preds_test, &quot;train_preds&quot; =&gt; preds_train, &quot;w&quot; =&gt; w, &quot;b&quot; =&gt; b, &quot;ğ›¼&quot; =&gt; ğ›¼, &quot;num_iterations&quot; =&gt; num_iterations ) d; end . d = model(X_train, y_train, X_test, y_test, 2000, 0.005, true) . begin x = 1:2000; y = d[&quot;costs&quot;]; gr() # backend plot(x, y, title = &quot;Learning rate = 0.005&quot;, label=&quot;negative log-likelihood&quot;) xlabel!(&quot;iteration&quot;) ylabel!(&quot;cost&quot;) end .",
            "url": "https://abhiswain97.github.io/PGDiplomaUOH/2021/03/07/LR_Pluto_notebook.html",
            "relUrl": "/2021/03/07/LR_Pluto_notebook.html",
            "date": " â€¢ Mar 7, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Co-ordinate Geometry and Linear Algebra",
            "content": "Equation of a line . plot_line() . $$ Large y = mx + c$$ . The equation given is the equation for a straight line. m -&gt; slope, c -&gt; y-intercept, x -&gt; x-coordinate . Points to be noted . Slope is same everywhere on a line | Two parellel lines have the same slope | Intersecting lines have different slopes | General form of the equation of a straight line . $ large ax + by + c = 0 $ . Rearranging to get it into the slope - intercept form gives: . $ large y = frac{-c-ax}{b} = frac{-a}{b}x + frac{-c}{b} $ . Here, m = $ Large frac{-a}{b}$ &amp; c = $ Large frac{-c}{b}$ . If we generalize the equation to n dimensions . $ large w_1x_1 + w_2x_2 + dots + w_0 = 0 $ . $ large x_1, x_2, dots large w_1, w_2, dots $ . Equation of a plane . $ large pi : ax + by + cz + d = 0 large L: ax + by + c = 0 $ . $ large pi$ is used to represent plane . We can also write it as a more general notation: . $ large pi : w_1x_1 + w_2x_2 + w_3x_3 + w_0 = 0$ . Equation of a hyperplane . When the number of dimensions is greater than 2, then we call it hyper-plane It is represented by: $ large pi_d$ . It&#39;s equation is: . $ large pi_d : w_1x_1 + w_2x_2 + dots + w_dx_d + w_0= 0$ . Vectors in Linear Algebra . display_image(r&quot;C: Users Abhishek Swain Desktop PGD_UOH imgs vector.png&quot;, size=None) . The above picture is what a vector is. It has a magnitude and a direction. . The magnitude of the above vector is calculated by: $ large || vec u|| = sqrt{a^2 + b^2}$. This is a 2-d vector. . For a &#39;d&#39; dimensional vector the formula is generalized to: $ large || vec x|| = sqrt{x_1^2 + x_2^2 + dots + x_d^2}$ . The equation of a hyperplane: $ large pi_d : w_1x_1 + w_2x_2 + dots + w_dx_d + w_0= 0$ . can be rewritten in a matrix form using Linear algebra: . $ large begin{bmatrix} w_1 dots w_d end{bmatrix} begin{bmatrix} x_1 vdots x_d end{bmatrix} + w_0 = 0$ . $ large W in R^d$, which means W is a d-dimensional vector and all it&#39;s values are real numbers. $ large w_0 $ is a scalar. . The Vector with all $ large w$s is a row vector &amp; the vector with all $ large x $s is a column vector In maths, when we say vector we mean a column vector by convention . We can write this in a more concise way: $ large W^Tx + w_0 = 0$ . Dataset . df.head() . Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species . 0 1 | 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa | . 1 2 | 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa | . 2 3 | 4.7 | 3.2 | 1.3 | 0.2 | Iris-setosa | . 3 4 | 4.6 | 3.1 | 1.5 | 0.2 | Iris-setosa | . 4 5 | 5.0 | 3.6 | 1.4 | 0.2 | Iris-setosa | . The above is the example of a dataset. This one is the famous Iris dataset.Iris dataset contains 4 features(SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm) and a target(3 types of flowers) Each row has 4 columns representing the features and the last column is the target(the thing we want to predict) . We can have a concise mathematical representation of a dataset as: . $ large D = {(x_i, y_i); x_i in R^d, y_i in {c_1, c_2} }$ . $ large x_i$ represents one feature vector(one row of columns except the target ones) $ large y_i$ represents one target vector . Angle between Vectors . display_image(fname=r&quot;C: Users Abhishek Swain Desktop PGD_UOH imgs Angle-Between-Two-Vectors.png&quot;, size=None) . Here we need the notion of a dot product. This is taken from Wikipedia . In mathematics, the dot product or scalar product is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors), and returns a single number. In Euclidean geometry, the dot product of the Cartesian coordinates of two vectors is widely used. It is often called &quot;the&quot; inner product (or rarely projection product) of Euclidean space, even though it is not the only inner product that can be defined on Euclidean space (see Inner product space for more). Algebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers. Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them. . From the last line of the definition, . $ large vec A cdot vec B = | vec A|| vec B| cos theta = vec A ^ T vec B$ . After rearranging, . $ Large theta = cos ^ {-1} left ( frac{ vec A cdot vec B}{| vec A|| vec B|} right )$ . If $ large theta$ is 0 then, vectors are perpendicular . Special cases of equation of a plane . One more concept we need is the concept of a unit vector. A unit vector is a vector whose magnitude is 1. Represented by $ large hat x $ called the &quot;hat&quot; symbol. . 1. Passing through origin . Equation recap: $ large pi : w_1x_1 + w_2x_2 + w_3x_3 + w_0 = 0$ . for the plane to pass through origin, $ large x_1, x_2, x_3 = 0$. Plugging these values in the equation we have: $w_0 = 0$. This is the equation of plane passing through origin . 2. Not passing through origin . display_image(r&quot;C: Users Abhishek Swain Desktop PGD_UOH imgs plane-not-through-origin.jpg&quot;) . Equation of plane in vector form: . $ large w^Tx + w_0 = 0 $ $ large =&gt; ||w||||x|| cos theta = 0 $ $ large =&gt; ||x|| cos theta + w_0 = 0 $ (as $ large w $ is a unit vector) $ large ||x|| cos theta = a $ (from the figure) . So, finally $ large a = -w_0$ . As you can see, $ large a$ is the distance of the plane from the origin. So, what we see is the distance of a plane not passing through origin is $ large |w_0|$ provided $ large w$ is a unit vector. . if $ large w$ is not a unit vector then $ large a = frac{|w_0|}{||w||}$ . We can prove it like this: $ large w cdot x + w_0 = 0$ $ large =&gt; ||w||||x|| cos theta + w_0 = 0 $ $ large =&gt; ||w|| a + w_0 = 0 $, as $ large ||x|| cos theta = a$ (from the figure) $ large =&gt; a = frac{-w_0}{||w||} $ . but ofcourse we can do away with the -ve sign as distance can&#39;t be negative. So, $ large a = frac{|w_0|}{||w||} $ . Half Spaces .",
            "url": "https://abhiswain97.github.io/PGDiplomaUOH/machine%20learning/maths/2021/03/03/Co-ordinate-Geometry.html",
            "relUrl": "/machine%20learning/maths/2021/03/03/Co-ordinate-Geometry.html",
            "date": " â€¢ Mar 3, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.Â &#8617; . |",
          "url": "https://abhiswain97.github.io/PGDiplomaUOH/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://abhiswain97.github.io/PGDiplomaUOH/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}